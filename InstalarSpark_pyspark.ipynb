{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pasos para el setting up de Spark en local\n",
    "1. Ensure that Java is installed; otherwise install Java.\n",
    "2. Download the latest version of Apache Spark from\n",
    "https://spark.apache.org/downloads.html.\n",
    "Extrae el tgz y tar hasta que quede solo la carpeta con los archivos de spark\n",
    "3. Extract the files from the zipped folder.\n",
    "4. Copy all the Spark-related files to their respective directory.\n",
    "ademas si estas en windows debes anañir a la carpeta ej: C:\\spark\\spark-2.4.4-bin-hadoop2.7\\bin winutils.exe para la version de hadoop\n",
    "5. Configure the environment variables to be able to run Spark. Variables del entorno globales\n",
    "`SPARK_HOME  = C:\\spark\\spark-2.4.4-bin-hadoop2.7\n",
    "HADOOP_HOME = C:\\spark\\spark-2.4.4-bin-hadoop2.7`\n",
    "6. Verify the installation and run Spark. con los siguientes chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark\\\\spark-2.4.4-bin-hadoop2.7'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comando para encontrar la distribucion \n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "findspark.find()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark\\\\spark-2.4.4-bin-hadoop2.7'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estos dos chunks de código son para la comprobar la configuración del spark en el sistema windows. Si no sale error es que todo ha salido bien. No son necesarios para funcionar el modulo pyspark\n",
    "import pyspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CARGA DE PYSPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# librerias necesarias para la sesión de spark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = pyspark.SparkConf().setAppName('appName').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "#    pyspark.sql.SparkSession es el punto de entrada principal para la funcionalidad DataFrame y SQL. \n",
    "#    PARA TERMINAR LA SESION --> con sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de librerias para pyspark FUNCIONES SQL y TIPOS DE DATOS\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para convertir fecha a timestamp con pandas_udf (calculo por vectores en lugar de linea a linea)\n",
    "#  MapType, ArrayType of TimestampType, and nested StructType. BinaryType is supported only when PyArrow is 0.10.0 or above.\n",
    "\n",
    "def date_convert(date_str): # atención en pandas udf no va bien. salta un error... revisar\n",
    "    date_format = '%m/%d/%Y %H:%M:%S'\n",
    "    try:\n",
    "        dt=pd.to_datetime(date_str,errors='raise',format=date_format)\n",
    "    except ValueError as v:\n",
    "        if len(v.args) > 0 and v.args[0].startswith('unconverted data remains: '):\n",
    "            dt = dt[:-(len(v.args[0])-26)]\n",
    "            dt=datetime.strptime(dt,date_format)\n",
    "        else:\n",
    "            raise v\n",
    "    return dt\n",
    "# La siguiente funcion da una salida de Timestamp a la funcion date_convert en spark\n",
    "date_convert_pudf = F.pandas_udf(date_convert, TimestampType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para convertir fecha a timestamp\n",
    "def date_convert(date_str):\n",
    "    date_format = '%m/%d/%Y %H:%M:%S'\n",
    "    try:\n",
    "        dt=pd.to_datetime(date_str,errors='raise',format=date_format)\n",
    "    except ValueError as v:\n",
    "        if len(v.args) > 0 and v.args[0].startswith('unconverted data remains: '):\n",
    "            dt = dt[:-(len(v.args[0])-26)]\n",
    "            dt=datetime.strptime(dt,date_format)\n",
    "        else:\n",
    "            raise v\n",
    "    return dt\n",
    "# La siguiente funcion da una salida de Timestamp a la funcion date_convert en spark\n",
    "date_convert_udf = F.udf(date_convert, TimestampType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  dt,ht = date_str.split()\n",
    "dt , ht ='04/01/2014 00:11:00'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2014-04-01 00:11:00')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_datetime('4/1/2014 0:11:00', format= '%m/%d/%Y %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2014-04-01 00:11:00')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_convert('04/01/2014 00:11:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+--------+------+\n",
      "|       Date/Time|    Lat|     Lon|  Base|\n",
      "+----------------+-------+--------+------+\n",
      "|4/1/2014 0:11:00| 40.769|-73.9549|B02512|\n",
      "|4/1/2014 0:17:00|40.7267|-74.0345|B02512|\n",
      "|4/1/2014 0:21:00|40.7316|-73.9873|B02512|\n",
      "|4/1/2014 0:28:00|40.7588|-73.9776|B02512|\n",
      "|4/1/2014 0:33:00|40.7594|-73.9722|B02512|\n",
      "|4/1/2014 0:33:00|40.7383|-74.0403|B02512|\n",
      "|4/1/2014 0:39:00|40.7223|-73.9887|B02512|\n",
      "|4/1/2014 0:45:00| 40.762| -73.979|B02512|\n",
      "|4/1/2014 0:55:00|40.7524| -73.996|B02512|\n",
      "|4/1/2014 1:01:00|40.7575|-73.9846|B02512|\n",
      "|4/1/2014 1:19:00|40.7256|-73.9869|B02512|\n",
      "|4/1/2014 1:48:00|40.7591|-73.9684|B02512|\n",
      "|4/1/2014 1:49:00|40.7271|-73.9803|B02512|\n",
      "|4/1/2014 2:11:00|40.6463|-73.7896|B02512|\n",
      "|4/1/2014 2:25:00|40.7564|-73.9167|B02512|\n",
      "|4/1/2014 2:31:00|40.7666|-73.9531|B02512|\n",
      "|4/1/2014 2:43:00| 40.758|-73.9761|B02512|\n",
      "|4/1/2014 3:22:00|40.7238|-73.9821|B02512|\n",
      "|4/1/2014 3:35:00|40.7531|-74.0039|B02512|\n",
      "|4/1/2014 3:35:00|40.7389|-74.0393|B02512|\n",
      "+----------------+-------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# leer un csv\n",
    "\n",
    "uberpickabr14 = spark.read.csv(\n",
    "    'file:/c:/users/lucatic/documents/datasets/uber_pickups14/uber-raw-data-apr14.csv',\n",
    "    inferSchema=True,header=True\n",
    ")\n",
    "uberpickabr14.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date/Time: string (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Lon: double (nullable = true)\n",
      " |-- Base: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ver la estructura del dataset\n",
    "uberpickabr14.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "564516"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uberpickabr14.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+-------------------+-------------------+------+\n",
      "|summary|       Date/Time|                Lat|                Lon|  Base|\n",
      "+-------+----------------+-------------------+-------------------+------+\n",
      "|  count|          564516|             564516|             564516|564516|\n",
      "|   mean|            null|  40.74000520746832| -73.97681683903477|  null|\n",
      "| stddev|            null|0.03608320502016508|0.05042582837278078|  null|\n",
      "|    min|4/1/2014 0:00:00|            40.0729|           -74.7733|B02512|\n",
      "|    25%|            null|            40.7225|           -73.9977|  null|\n",
      "|    50%|            null|            40.7425|           -73.9847|  null|\n",
      "|    75%|            null|            40.7607|             -73.97|  null|\n",
      "|    max|4/9/2014 9:59:00|            42.1166|           -72.0666|B02764|\n",
      "+-------+----------------+-------------------+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uberpickabr14.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Funciones definidas por usuario en pandas y que puedes ejecutar con spark\n",
    "# Declaramos la orden\n",
    "\n",
    "\n",
    "# date_convert_pudf() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|date_convert(Date/Time)|\n",
      "+-----------------------+\n",
      "|    2014-04-01 00:11:00|\n",
      "|    2014-04-01 00:17:00|\n",
      "|    2014-04-01 00:21:00|\n",
      "|    2014-04-01 00:28:00|\n",
      "|    2014-04-01 00:33:00|\n",
      "|    2014-04-01 00:33:00|\n",
      "|    2014-04-01 00:39:00|\n",
      "|    2014-04-01 00:45:00|\n",
      "|    2014-04-01 00:55:00|\n",
      "|    2014-04-01 01:01:00|\n",
      "|    2014-04-01 01:19:00|\n",
      "|    2014-04-01 01:48:00|\n",
      "|    2014-04-01 01:49:00|\n",
      "|    2014-04-01 02:11:00|\n",
      "|    2014-04-01 02:25:00|\n",
      "|    2014-04-01 02:31:00|\n",
      "|    2014-04-01 02:43:00|\n",
      "|    2014-04-01 03:22:00|\n",
      "|    2014-04-01 03:35:00|\n",
      "|    2014-04-01 03:35:00|\n",
      "+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uberpickabr14.select(date_convert_udf(uberpickabr14['Date/Time'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+-------------------+\n",
      "|       Date/Time|  Base|           datetime|\n",
      "+----------------+------+-------------------+\n",
      "|4/1/2014 0:11:00|B02512|2014-04-01 00:11:00|\n",
      "|4/1/2014 0:17:00|B02512|2014-04-01 00:17:00|\n",
      "|4/1/2014 0:21:00|B02512|2014-04-01 00:21:00|\n",
      "|4/1/2014 0:28:00|B02512|2014-04-01 00:28:00|\n",
      "|4/1/2014 0:33:00|B02512|2014-04-01 00:33:00|\n",
      "|4/1/2014 0:33:00|B02512|2014-04-01 00:33:00|\n",
      "+----------------+------+-------------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#seleccionamos y convertimos la columna\n",
    "uberpickabr14.select(uberpickabr14['Date/Time'],uberpickabr14.Base,\n",
    "    date_convert_udf(uberpickabr14['Date/Time']).alias('datetime')\n",
    ").show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:/c:/users/lucatic/documents/datasets/uber_pickups14/uber-raw-data-apr14.csv'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'file:/C:/Users/Lucatic/Documents/Datasets/UBER_PICKUPS14/uber-raw-data-apr14.csv'.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+-------+--------+-------------------+\n",
      "|       Date/Time|  Base|    Lat|     Lon|           datetime|\n",
      "+----------------+------+-------+--------+-------------------+\n",
      "|4/1/2014 0:11:00|B02512| 40.769|-73.9549|2014-04-01 00:11:00|\n",
      "|4/1/2014 0:17:00|B02512|40.7267|-74.0345|2014-04-01 00:17:00|\n",
      "|4/1/2014 0:21:00|B02512|40.7316|-73.9873|2014-04-01 00:21:00|\n",
      "|4/1/2014 0:28:00|B02512|40.7588|-73.9776|2014-04-01 00:28:00|\n",
      "|4/1/2014 0:33:00|B02512|40.7594|-73.9722|2014-04-01 00:33:00|\n",
      "+----------------+------+-------+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## CAMBIAR DE NOMBRE LAS COLUMNAS\n",
    "UBERPICKS = uberpickabr14.select(\n",
    "    uberpickabr14['Date/Time'],\n",
    "    uberpickabr14.Base,\n",
    "    uberpickabr14.Lat,\n",
    "    uberpickabr14.Lon,\n",
    "    date_convert_udf(uberpickabr14['Date/Time']).alias('datetime') #Alias para renombrar col\n",
    ")\n",
    "UBERPICKS.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+--------+------+-------------------+\n",
      "|        FechaStr|    Lat|     Lon|  Base|           datetime|\n",
      "+----------------+-------+--------+------+-------------------+\n",
      "|4/1/2014 0:11:00| 40.769|-73.9549|B02512|2014-04-01 00:11:00|\n",
      "|4/1/2014 0:17:00|40.7267|-74.0345|B02512|2014-04-01 00:17:00|\n",
      "|4/1/2014 0:21:00|40.7316|-73.9873|B02512|2014-04-01 00:21:00|\n",
      "|4/1/2014 0:28:00|40.7588|-73.9776|B02512|2014-04-01 00:28:00|\n",
      "|4/1/2014 0:33:00|40.7594|-73.9722|B02512|2014-04-01 00:33:00|\n",
      "+----------------+-------+--------+------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## lo de arriba pero más sencillo\n",
    "## CAMBIAR DE NOMBRE LAS COLUMNAS y modificarlas.\n",
    "uberpickabr14.withColumn( # es como un mutate\n",
    "    'datetime',date_convert_udf(uberpickabr14['Date/Time']))\\\n",
    ".withColumnRenamed(\"Date/Time\",\"FechaStr\")\\\n",
    ".show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para devolver el día de la semana. \n",
    "def dayweek(col):\n",
    "    return col.strftime('%A')\n",
    "dayweek_udf = F.udf(dayweek, StringType()) #UDF es mas lenta porque lo hace linea a linea, pandas_udf lo verctoriza, pero ... no va bien, hay que programarlo de manera mas complicada. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+-------+--------+-------------------+-----+-------+\n",
      "|       Date/Time|  Base|    Lat|     Lon|           datetime|FECHA|      D|\n",
      "+----------------+------+-------+--------+-------------------+-----+-------+\n",
      "|4/1/2014 0:11:00|B02512| 40.769|-73.9549|2014-04-01 00:11:00|01-04|Tuesday|\n",
      "|4/1/2014 0:17:00|B02512|40.7267|-74.0345|2014-04-01 00:17:00|01-04|Tuesday|\n",
      "|4/1/2014 0:21:00|B02512|40.7316|-73.9873|2014-04-01 00:21:00|01-04|Tuesday|\n",
      "|4/1/2014 0:28:00|B02512|40.7588|-73.9776|2014-04-01 00:28:00|01-04|Tuesday|\n",
      "|4/1/2014 0:33:00|B02512|40.7594|-73.9722|2014-04-01 00:33:00|01-04|Tuesday|\n",
      "|4/1/2014 0:33:00|B02512|40.7383|-74.0403|2014-04-01 00:33:00|01-04|Tuesday|\n",
      "|4/1/2014 0:39:00|B02512|40.7223|-73.9887|2014-04-01 00:39:00|01-04|Tuesday|\n",
      "|4/1/2014 0:45:00|B02512| 40.762| -73.979|2014-04-01 00:45:00|01-04|Tuesday|\n",
      "|4/1/2014 0:55:00|B02512|40.7524| -73.996|2014-04-01 00:55:00|01-04|Tuesday|\n",
      "|4/1/2014 1:01:00|B02512|40.7575|-73.9846|2014-04-01 01:01:00|01-04|Tuesday|\n",
      "|4/1/2014 1:19:00|B02512|40.7256|-73.9869|2014-04-01 01:19:00|01-04|Tuesday|\n",
      "|4/1/2014 1:48:00|B02512|40.7591|-73.9684|2014-04-01 01:48:00|01-04|Tuesday|\n",
      "|4/1/2014 1:49:00|B02512|40.7271|-73.9803|2014-04-01 01:49:00|01-04|Tuesday|\n",
      "|4/1/2014 2:11:00|B02512|40.6463|-73.7896|2014-04-01 02:11:00|01-04|Tuesday|\n",
      "|4/1/2014 2:25:00|B02512|40.7564|-73.9167|2014-04-01 02:25:00|01-04|Tuesday|\n",
      "|4/1/2014 2:31:00|B02512|40.7666|-73.9531|2014-04-01 02:31:00|01-04|Tuesday|\n",
      "|4/1/2014 2:43:00|B02512| 40.758|-73.9761|2014-04-01 02:43:00|01-04|Tuesday|\n",
      "|4/1/2014 3:22:00|B02512|40.7238|-73.9821|2014-04-01 03:22:00|01-04|Tuesday|\n",
      "|4/1/2014 3:35:00|B02512|40.7531|-74.0039|2014-04-01 03:35:00|01-04|Tuesday|\n",
      "|4/1/2014 3:35:00|B02512|40.7389|-74.0393|2014-04-01 03:35:00|01-04|Tuesday|\n",
      "+----------------+------+-------+--------+-------------------+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "UBERPICKS = UBERPICKS\\\n",
    ".withColumn('FECHA',F.date_format('datetime','dd-MM' ))\\\n",
    ".withColumn('D',dayweek_udf('datetime'))\n",
    "\n",
    "UBERPICKS.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FILTROS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+-------+--------+-------------------+\n",
      "|       Date/Time|  Base|    Lat|     Lon|           datetime|\n",
      "+----------------+------+-------+--------+-------------------+\n",
      "|4/2/2014 2:33:00|B02512|40.7383|-73.9839|2014-04-02 02:33:00|\n",
      "|4/2/2014 3:18:00|B02512|40.7339|-73.9949|2014-04-02 03:18:00|\n",
      "|4/2/2014 3:33:00|B02512|40.7402|-74.0059|2014-04-02 03:33:00|\n",
      "|4/2/2014 3:43:00|B02512|40.7731|-73.9584|2014-04-02 03:43:00|\n",
      "|4/2/2014 3:49:00|B02512|40.6461|-73.7769|2014-04-02 03:49:00|\n",
      "|4/2/2014 3:56:00|B02512|40.7475|-73.9565|2014-04-02 03:56:00|\n",
      "|4/2/2014 4:18:00|B02512|40.7471|-74.0023|2014-04-02 04:18:00|\n",
      "|4/2/2014 4:22:00|B02512|40.7816| -73.956|2014-04-02 04:22:00|\n",
      "|4/2/2014 4:25:00|B02512|40.7788|-73.9481|2014-04-02 04:25:00|\n",
      "|4/2/2014 4:26:00|B02512|40.6948|-74.1779|2014-04-02 04:26:00|\n",
      "|4/2/2014 4:29:00|B02512|40.7583|-73.9665|2014-04-02 04:29:00|\n",
      "|4/2/2014 4:36:00|B02512|40.7258|-73.9877|2014-04-02 04:36:00|\n",
      "|4/2/2014 4:40:00|B02512|40.7655|-73.9932|2014-04-02 04:40:00|\n",
      "|4/2/2014 4:51:00|B02512|40.7863| -73.956|2014-04-02 04:51:00|\n",
      "|4/2/2014 4:55:00|B02512| 40.729|-74.0328|2014-04-02 04:55:00|\n",
      "|4/2/2014 4:58:00|B02512|40.7445|-73.9838|2014-04-02 04:58:00|\n",
      "|4/2/2014 5:06:00|B02512|40.7349|-73.9891|2014-04-02 05:06:00|\n",
      "|4/2/2014 5:10:00|B02512|40.7657|-73.9629|2014-04-02 05:10:00|\n",
      "|4/2/2014 5:12:00|B02512| 40.645|-73.7818|2014-04-02 05:12:00|\n",
      "|4/2/2014 5:13:00|B02512|40.8092|-73.9539|2014-04-02 05:13:00|\n",
      "+----------------+------+-------+--------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Filtrar por fechas\n",
    "UBERPICKS.where(\n",
    "    UBERPICKS.datetime > datetime(2014,4,2,2,30) \n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+-------+--------+-------------------+\n",
      "|       Date/Time|  Base|    Lat|     Lon|           datetime|\n",
      "+----------------+------+-------+--------+-------------------+\n",
      "|4/1/2014 0:11:00|B02512| 40.769|-73.9549|2014-04-01 00:11:00|\n",
      "|4/1/2014 0:17:00|B02512|40.7267|-74.0345|2014-04-01 00:17:00|\n",
      "|4/1/2014 0:21:00|B02512|40.7316|-73.9873|2014-04-01 00:21:00|\n",
      "|4/1/2014 0:28:00|B02512|40.7588|-73.9776|2014-04-01 00:28:00|\n",
      "|4/1/2014 0:33:00|B02512|40.7594|-73.9722|2014-04-01 00:33:00|\n",
      "+----------------+------+-------+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtro múltiple de dias\n",
    "UBERPICKS\\\n",
    ".filter(\n",
    "    UBERPICKS.datetime <= datetime(2014,4,2,5))\\\n",
    ".filter(\n",
    "    UBERPICKS.datetime > datetime(2014,4,1) ).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UBERPICKS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No nos olvidemos de acabar con la sesión spark!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prueba sinple de que Spark funciona. Calculo numero PI.\n",
    "import random\n",
    "\n",
    "num_samples = 100000000\n",
    "\n",
    "def inside(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "\n",
    "print(pi)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
